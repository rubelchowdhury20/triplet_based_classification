{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dfbbc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d86d853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "891dc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f1e39580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mold_dataset_all\u001b[0m/  \u001b[01;34mrefined_dataset\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls \"./dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "02813270",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"./dataset/refined_dataset/train/\"\n",
    "image_list = [os.path.join(root, name)\n",
    "            for root, dirs, files in os.walk(src_path)\n",
    "            for name in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "76c6185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_list)\n",
    "random.shuffle(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0749a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7a752885",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {'batch_size': 8,\n",
    "            'shuffle': False,\n",
    "            'num_workers': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1ac4cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d02c0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, images, transforms=None):\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.images[index])\n",
    "        image = np.asarray(image)\n",
    "        image = image[:,:,:3]\n",
    "        image = Image.fromarray(image)\n",
    "        return self.images[index], self.transforms(image)\n",
    "#         if self.transforms is not None:\n",
    "#             try:\n",
    "#                 return self.images[index], self.transforms(image)\n",
    "#             except:\n",
    "#                 print(self.images[index])\n",
    "        return self.images[index], image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5c1f1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(image_list, data_transform)\n",
    "data_loader = data.DataLoader(dataset, **PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1772bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained model and generating embedding based on that\n",
    "base_model = models.resnet18(pretrained=False).to(DEVICE)\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = base_model.fc.in_features\n",
    "base_model.fc = nn.Sequential(nn.Linear(num_ftrs, 256), nn.Linear(256, 128))\n",
    "base_model = base_model.to(DEVICE)\n",
    "\n",
    "# loading the trained model with trained weights\n",
    "checkpoint = torch.load(\"./weights/refined_weights/model_best.pth\")\n",
    "base_model.load_state_dict(checkpoint['state_dict'])\n",
    "base_model = base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "81af2a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annoy_index(image_embeddings, embedding_size):\n",
    "    index_to_label = {}\n",
    "    annoy_index = AnnoyIndex(embedding_size, metric=\"euclidean\")\n",
    "    for index, embedding in tqdm(enumerate(image_embeddings)):\n",
    "        index_to_label[index] = embedding[\"image\"].split(\"/\")[-2]\n",
    "        annoy_index.add_item(index, embedding[\"embedding\"])\n",
    "    annoy_index.build(10000)\n",
    "    return annoy_index, index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f2c45424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(emb_dataloader):\n",
    "    embeddings = []\t\t\t\t# list to store the embeddings in dict format as name, embedding\n",
    "    base_model.eval()\n",
    "    with torch.no_grad():\t\t\t\t# no update of parameters\n",
    "        for image_names, images in tqdm(emb_dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            image_embeddings = base_model(images)\n",
    "            embeddings.extend([{\"image\": image_names[index], \"embedding\": embedding} for index, embedding in enumerate(image_embeddings.cpu().data)])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f49616e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [00:04<00:00, 67.39it/s]\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = get_embeddings(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ef7deaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2200it [00:00, 4961.42it/s]\n"
     ]
    }
   ],
   "source": [
    "annoy_index, annoy_index_to_label = create_annoy_index(image_embeddings, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "50097cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index.save(\"annoy_index.ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c081417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('annoy_index_to_label.json', 'w') as f:\n",
    "    json.dump(annoy_index_to_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71090fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8c26ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img_name = './dataset/old_dataset_all/old_dataset_2/val_retina/4.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ee1d5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = base_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8e5d38a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_img = Image.open(query_img_name)\n",
    "query_img = data_transform(query_img)\n",
    "query_img = query_img.unsqueeze(0)\n",
    "query_img_embedding = base_model(query_img)\n",
    "query_img_embedding = query_img_embedding.squeeze()\n",
    "query_img_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a324f2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2999,  0.3010,  0.5467, -0.0063, -0.5192, -0.0473, -0.4775, -0.1055,\n",
       "         0.1031, -0.0983,  0.1274, -0.2049, -0.5786, -0.4274, -0.3502, -0.1535,\n",
       "        -0.2950, -0.6859, -0.5819,  0.8397, -0.1136,  0.6252,  0.0801,  0.0777,\n",
       "        -0.3418,  0.0991, -0.4070,  0.2345, -0.9932, -0.1629, -0.0989,  0.3058,\n",
       "         0.5070,  0.1654, -0.8238,  0.6589, -0.2118,  0.2043, -0.4324, -0.3545,\n",
       "         0.1861,  0.1656,  0.2705,  0.6299,  0.6270,  0.0326, -0.2639,  0.0637,\n",
       "        -0.1423,  0.3423, -0.0616, -0.1244,  0.3716,  0.2894,  0.0755, -0.0934,\n",
       "         0.4488, -0.3963,  0.2110, -0.7865,  0.4953,  0.5968,  0.0142, -0.3433,\n",
       "         0.3595, -0.2530,  0.0385,  0.9638,  0.1151,  0.0786, -0.0301,  0.1556,\n",
       "        -0.0017,  0.1398, -0.3046,  0.8517,  0.1961,  0.3400, -0.1359,  0.3403,\n",
       "        -0.2605,  0.8044, -0.3182, -0.1349,  0.1944, -0.0462, -0.2366,  0.5896,\n",
       "        -0.1673, -0.6148, -0.2910,  0.2694, -0.3890, -0.0062, -0.3453, -0.0335,\n",
       "        -0.2148, -0.2522, -0.3993, -0.1779, -0.1169, -0.5953,  0.0287,  0.0764,\n",
       "        -0.2671, -0.0234, -0.1850,  0.8543, -0.4748,  0.4362,  0.0450, -0.1507,\n",
       "        -0.0203, -0.6636, -0.4862, -0.5025,  0.5157, -0.0071,  0.1728,  0.6333,\n",
       "         0.4751,  0.1543, -1.0588,  0.2816, -1.0448,  0.3950,  0.3168, -0.5746],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "77e3495d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2999,  0.3010,  0.5467, -0.0063, -0.5192, -0.0473, -0.4775, -0.1055,\n",
       "         0.1031, -0.0983,  0.1274, -0.2049, -0.5786, -0.4274, -0.3502, -0.1535,\n",
       "        -0.2950, -0.6859, -0.5819,  0.8397, -0.1136,  0.6252,  0.0801,  0.0777,\n",
       "        -0.3418,  0.0991, -0.4070,  0.2345, -0.9932, -0.1629, -0.0989,  0.3058,\n",
       "         0.5070,  0.1654, -0.8238,  0.6589, -0.2118,  0.2043, -0.4324, -0.3545,\n",
       "         0.1861,  0.1656,  0.2705,  0.6299,  0.6270,  0.0326, -0.2639,  0.0637,\n",
       "        -0.1423,  0.3423, -0.0616, -0.1244,  0.3716,  0.2894,  0.0755, -0.0934,\n",
       "         0.4488, -0.3963,  0.2110, -0.7865,  0.4953,  0.5968,  0.0142, -0.3433,\n",
       "         0.3595, -0.2530,  0.0385,  0.9638,  0.1151,  0.0786, -0.0301,  0.1556,\n",
       "        -0.0017,  0.1398, -0.3046,  0.8517,  0.1961,  0.3400, -0.1359,  0.3403,\n",
       "        -0.2605,  0.8044, -0.3182, -0.1349,  0.1944, -0.0462, -0.2366,  0.5896,\n",
       "        -0.1673, -0.6148, -0.2910,  0.2694, -0.3890, -0.0062, -0.3453, -0.0335,\n",
       "        -0.2148, -0.2522, -0.3993, -0.1779, -0.1169, -0.5953,  0.0287,  0.0764,\n",
       "        -0.2671, -0.0234, -0.1850,  0.8543, -0.4748,  0.4362,  0.0450, -0.1507,\n",
       "        -0.0203, -0.6636, -0.4862, -0.5025,  0.5157, -0.0071,  0.1728,  0.6333,\n",
       "         0.4751,  0.1543, -1.0588,  0.2816, -1.0448,  0.3950,  0.3168, -0.5746],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_img_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "aba8bbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad',\n",
       " 'salad']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_images = annoy_index.get_nns_by_vector(query_img_embedding, 20, include_distances=True)\n",
    "similar_image_labels = [annoy_index_to_label[i] for i in similar_images[0]]\n",
    "similar_image_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08045434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([487,\n",
       "  435,\n",
       "  405,\n",
       "  526,\n",
       "  410,\n",
       "  452,\n",
       "  427,\n",
       "  492,\n",
       "  501,\n",
       "  421,\n",
       "  537,\n",
       "  446,\n",
       "  428,\n",
       "  542,\n",
       "  432,\n",
       "  426,\n",
       "  522,\n",
       "  407,\n",
       "  475,\n",
       "  456],\n",
       " [0.8291601538658142,\n",
       "  0.8713507056236267,\n",
       "  0.886089026927948,\n",
       "  0.9035285115242004,\n",
       "  0.9094564914703369,\n",
       "  0.9120723009109497,\n",
       "  0.9153848886489868,\n",
       "  0.9263986945152283,\n",
       "  0.9478886127471924,\n",
       "  0.9479461908340454,\n",
       "  0.9500370621681213,\n",
       "  0.9624344706535339,\n",
       "  0.9677736759185791,\n",
       "  0.9706259369850159,\n",
       "  0.9721582531929016,\n",
       "  0.9772781133651733,\n",
       "  0.986505389213562,\n",
       "  0.9901344180107117,\n",
       "  0.9902897477149963,\n",
       "  0.995437502861023])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b8d147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4859db75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
